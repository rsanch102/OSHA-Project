---
title: "Investigating which businesses are inspected by OSHA; which industries are most dangerous?"
output:
  html_document: default
  word_document: default
date: "2024-06-05"
---

![](images/https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.arkansasonline.com%2Flisten%2F&psig=AOvVaw2g9k1NKXCF61O5S28-44_c&ust=1717728305432000&source=images&cd=vfe&opi=89978449&ved=0CBIQjRxqFwoTCPih5d76xYYDFQAAAAAdAAAAABAQ.png)

**Memo**

This would be an investigation into Occupational Safety and Health Administration (OSHA) data in Arkansas to determine which industries receive the most inspections and which kinds. Then consequently, use the inspection data to investigate these reports, which industries or companies have the most severe violations, or complaints per business. 

Construction and poultry businesses are the industries with the most fatalities and severe injuries. There is a pattern of workers falling off of heights of more than 14ft because of dangerous conditions and no guardrails.Employers are ignoring OSHA regulations, and I want to explore how often OSHA is following up with potentially violating businesses.

```{r setup, include=FALSE}
#I loaded two libraries into R, ###httr ### hot and rvest, to pull a http and scrape data from a webpage. 

# Then I loaded the headers to bypass the ‘403’ authentication error when loading the osha.gov website. The header which was partially written by Sean, Austin, Chatgpt, and myself was to convince the website that I was not a bot, but a person submitting a request and looking for the data.  

# By having the header defined, a 403 code no longer appeared, but a 304 request did. I then had to modify my header by changing the value of the cookie that the site was requesting, I just changed the value to equal ‘1’. Also I then defined the url with the OSHA query using two variables, State=Arkansas, Office=Little Rock, and I also was looking for Fed/State data, dating from 2024 back to the given search date of 2019. 

```

```{r}
library(httr)
library(rvest)
library(tidyverse)
library(janitor)
library(readxl)
library(lubridate)
```

```{r}
# Define the headers
headers <- c(
  "Accept" = "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,/;q=0.8,application/signed-exchange;v=b3;q=0.7",
  "Accept-Encoding" = "gzip, deflate, br, zstd",
  "Accept-Language" = "en-US,en;q=0.9",
  "Cache-Control" = "max-age=0",
  "Cookie" = "_gid=1",
  "If-Modified-Since" = "Mon, 03 Jun 2024 13:39:50 GMT",
  "If-None-Match" = "\"1717421990\"",
  "Priority" = "u=0, i",
  "Sec-Ch-Ua" = "\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"",
  "Sec-Ch-Ua-Mobile" = "?0",
  "Sec-Ch-Ua-Platform" = "\"macOS\"",
  "Sec-Fetch-Dest" = "document",
  "Sec-Fetch-Mode" = "navigate",
  "Sec-Fetch-Site" = "none",
  "Sec-Fetch-User" = "?1",
  "Upgrade-Insecure-Requests" = "1",
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
)

# The url only displayed the first 20 results of the 1,327 results, but by modifying the url to change the violations of the page results to show ‘2000’ instead. You can see and thus scrape all 1,327 results instantly without having to break it up by 63 pages.
# “show=2000&p_violations_exist=both”

# Define the URL
url <- "https://www.osha.gov/ords/imis/establishment.search?establishment=&state=AR&officetype=all&office=627100&sitezip=100000&startmonth=01&startday=01&startyear=2014&endmonth=06&endday=05&endyear=2024&p_case=all&p_violations_exist=both&p_start=&p_finish=20&p_sort=12&p_desc=DESC&p_direction=Next&p_show=3500"

# Make the GET request with headers
response <- GET(url, add_headers(.headers = headers))

# Check the status code
if (status_code(response) == 200) {
  # If the request is successful, parse the HTML content
  content <- content(response, as = "text")
  webpage <- read_html(content)
  
  # Extract all tables
  tables <- webpage %>%
    html_nodes("table") %>%
    html_table(fill = TRUE)
  
  # Then, because the scraper was only looking for the first table, I had to change the code to find the second table that had all of the relevant data, after inspecting the html code. 
  
  # Check if there are at least two tables
  if (length(tables) >= 2) {
    # Print the second table
    cleaned_table <- tables[[2]]
    print(tables[[2]])
  } else {
    print("Less than two tables found on the page.")
  }
} else {
  print("Failed to fetch the page.")
}

#rsw comment_fix your names:
cleaned_table <- cleaned_table %>% 
  clean_names()

```

```{r}
write.csv(cleaned_table,"osha_table.csv")
```


```{r}
# Load the OSHA data
cleaned_table <- read.csv("osha_table.csv")

# Select relevant columns and clean names
relevant_osha <- cleaned_table %>% 
  select(date_opened, naics, establishment_name, type) %>% 
  mutate(naics1 = as.character(naics))

# Load the NAICS data
naics <- read_excel("/Users/rachellsanchez/Desktop/DJNF_Merrill/OSHA Project/NAICS_codes.xlsx") %>% 
  clean_names() %>% 
  rename(naics1 = x2022_naics_us_code, industry = x2022_naics_us_title)

# Join the OSHA data with the NAICS data
joined_table <- relevant_osha %>%
  inner_join(naics, by = c("naics1" = "naics1"))

# Find the count of inspections per industry
inspection_counts <- joined_table %>% 
  count(industry) %>% 
  arrange(desc(n))

```
```{r} {r, results= 'asis'}
cat('<div class="flourish-embed flourish-parliament" data-src="visualisation/18261633"><script src="https://public.flourish.studio/resources/embed.js"></script></div>')
```


```{r}
# Filter for Fat/Cat incidents
fatalities_industry <- relevant_osha %>%
  filter(type == 'Fat/Cat') %>%
  select(establishment_name, type, naics1)

# Join with NAICS data to get industry names
fatalities_industry <- fatalities_industry %>%
  inner_join(naics, by = "naics1")

# Count Fat/Cat incidents per industry
fatcat_counts <- fatalities_industry %>% 
  count(industry) %>% 
  arrange(desc(n))

print(fatcat_counts)

# Save the Fat/Cat data to a CSV
write.csv(fatalities_industry, "fatalities_industry.csv")

```
```{r}
joined_table %>% 
  mutate(date1=mdy(date_opened)) %>% 
  mutate(year=year(date1)) %>% 
  filter(industry=="Poultry Processing") %>% 
  filter(type=="Fat/Cat") %>% 
  group_by(year) %>% 
  count(year)
```

```{r}
joined_table %>% 
  mutate(date1=mdy(date_opened)) %>% 
  mutate(year=year(date1)) %>% 
  filter(industry=="Electrical Contractors and Other Wiring Installation Contractors") %>% 
  filter(type=="Fat/Cat") %>% 
  group_by(year) %>% 
  count(year)
```

```{r}
joined_table %>% 
  mutate(date1=mdy(date_opened)) %>% 
  mutate(year=year(date1)) %>% 
  filter(industry=="Highway, Street, and Bridge Construction") %>% 
  filter(type=="Fat/Cat") %>% 
  group_by(year) %>% 
  count(year)
```

